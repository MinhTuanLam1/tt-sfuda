{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e5dec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from TT_SFUDA_2D.dataset import Dataset\n",
    "from TT_SFUDA_2D.metrics import iou_score\n",
    "from collections import OrderedDict\n",
    "\n",
    "from albumentations import RandomRotate90,Resize\n",
    "from albumentations.augmentations import transforms\n",
    "from albumentations.core.composition import Compose\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.jit\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as st_transforms\n",
    "\n",
    "from TT_SFUDA_2D.losses import *\n",
    "from TT_SFUDA_2D.archs import *\n",
    "from TT_SFUDA_2D.utils import *\n",
    "# from TT_SFUDA_2D import archs, losses\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# ============= NEW ACADEMIC IMPROVEMENTS =============\n",
    "\n",
    "class MultiScaleConsistency:\n",
    "    def __init__(self, scales=[0.75, 1.0, 1.25]):\n",
    "        self.scales = scales\n",
    "    \n",
    "    def generate_multiscale_pseudo_labels(self, model, image):\n",
    "        \"\"\"Generate pseudo-labels at multiple scales for consistency\"\"\"\n",
    "        pseudo_labels = []\n",
    "        confidences = []\n",
    "        \n",
    "        h, w = image.shape[-2:]\n",
    "        \n",
    "        for scale in self.scales:\n",
    "            new_h, new_w = int(h * scale), int(w * scale)\n",
    "            if scale != 1.0:\n",
    "                scaled_img = F.interpolate(image, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
    "            else:\n",
    "                scaled_img = image\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred = model(scaled_img)\n",
    "                if scale != 1.0:\n",
    "                    pred = F.interpolate(pred, size=(h, w), mode='bilinear', align_corners=False)\n",
    "                \n",
    "                prob = torch.sigmoid(pred)\n",
    "                pseudo_labels.append(prob)\n",
    "                \n",
    "                # Confidence based on entropy\n",
    "                entropy = -(prob * torch.log(prob + 1e-8) + (1-prob) * torch.log(1-prob + 1e-8))\n",
    "                confidence = 1.0 - entropy / np.log(2)  # Normalize entropy\n",
    "                confidences.append(confidence)\n",
    "        \n",
    "        # Weighted ensemble based on confidence\n",
    "        weights = torch.stack(confidences)\n",
    "        weights = F.softmax(weights, dim=0)\n",
    "        \n",
    "        final_pseudo = sum(w * pl for w, pl in zip(weights, pseudo_labels))\n",
    "        final_confidence = weights.mean(dim=0)\n",
    "        \n",
    "        return final_pseudo, final_confidence\n",
    "\n",
    "class MCDropoutUncertainty:\n",
    "    def __init__(self, n_samples=5):  # Reduced for efficiency\n",
    "        self.n_samples = n_samples\n",
    "    \n",
    "    def enable_dropout_inference(self, model):\n",
    "        \"\"\"Enable dropout during inference for uncertainty estimation\"\"\"\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Dropout):\n",
    "                module.train()\n",
    "    \n",
    "    def estimate_uncertainty(self, model, input_data):\n",
    "        \"\"\"Estimate epistemic uncertainty using MC-Dropout\"\"\"\n",
    "        model.eval()\n",
    "        self.enable_dropout_inference(model)\n",
    "        \n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for _ in range(self.n_samples):\n",
    "                pred = torch.sigmoid(model(input_data))\n",
    "                predictions.append(pred)\n",
    "        \n",
    "        predictions = torch.stack(predictions)\n",
    "        mean_pred = predictions.mean(dim=0)\n",
    "        uncertainty = predictions.var(dim=0)\n",
    "        \n",
    "        return mean_pred, uncertainty\n",
    "\n",
    "class ProgressiveDomainAdaptation:\n",
    "    def __init__(self, initial_threshold=0.9, final_threshold=0.6, total_epochs=100):\n",
    "        self.initial_threshold = initial_threshold\n",
    "        self.final_threshold = final_threshold\n",
    "        self.total_epochs = total_epochs\n",
    "    \n",
    "    def get_confidence_threshold(self, epoch):\n",
    "        \"\"\"Gradually decrease confidence threshold\"\"\"\n",
    "        progress = min(epoch / self.total_epochs, 1.0)\n",
    "        threshold = self.initial_threshold - (self.initial_threshold - self.final_threshold) * progress\n",
    "        return max(threshold, self.final_threshold)\n",
    "\n",
    "def gaussian_kernel(x, y, gamma=1.0):\n",
    "    \"\"\"Gaussian RBF kernel for MMD\"\"\"\n",
    "    x_size = x.size(0)\n",
    "    y_size = y.size(0)\n",
    "    dim = x.size(1)\n",
    "    \n",
    "    x = x.unsqueeze(1).expand(x_size, y_size, dim)\n",
    "    y = y.unsqueeze(0).expand(x_size, y_size, dim)\n",
    "    \n",
    "    return torch.exp(-gamma * torch.pow((x - y), 2).sum(2))\n",
    "\n",
    "def mmd_loss(source_features, target_features, gamma=1.0):\n",
    "    \"\"\"Maximum Mean Discrepancy loss for domain alignment\"\"\"\n",
    "    xx = gaussian_kernel(source_features, source_features, gamma)\n",
    "    yy = gaussian_kernel(target_features, target_features, gamma)\n",
    "    xy = gaussian_kernel(source_features, target_features, gamma)\n",
    "    \n",
    "    return xx.mean() + yy.mean() - 2 * xy.mean()\n",
    "\n",
    "def feature_alignment_loss(source_features, target_features, feature_layers=[1, 2, 3]):\n",
    "    \"\"\"Multi-layer feature alignment using MMD\"\"\"\n",
    "    total_loss = 0\n",
    "    for i in feature_layers:\n",
    "        if i < len(source_features) and i < len(target_features):\n",
    "            src_feat = source_features[i].view(source_features[i].size(0), -1)\n",
    "            tgt_feat = target_features[i].view(target_features[i].size(0), -1)\n",
    "            total_loss += mmd_loss(src_feat, tgt_feat)\n",
    "    \n",
    "    return total_loss / len(feature_layers) if feature_layers else 0\n",
    "\n",
    "def confidence_weighted_loss(student_pred, teacher_pred, confidence, gamma=2.0):\n",
    "    \"\"\"Confidence-weighted loss with adaptive weighting\"\"\"\n",
    "    # Normalize confidence to [0, 1]\n",
    "    conf_norm = (confidence - confidence.min()) / (confidence.max() - confidence.min() + 1e-8)\n",
    "    weight = conf_norm ** gamma\n",
    "    \n",
    "    mse_loss = (student_pred - teacher_pred) ** 2\n",
    "    weighted_loss = weight * mse_loss\n",
    "    return weighted_loss.mean()\n",
    "\n",
    "# ============= ENHANCED EXISTING FUNCTIONS =============\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--source', default=None, help='model name')\n",
    "    # parser.add_argument('--target', default=None, help='model name')\n",
    "    # # Add new hyperparameters\n",
    "    # parser.add_argument('--multiscale', action='store_true', help='Use multiscale consistency')\n",
    "    # parser.add_argument('--mc_dropout', action='store_true', help='Use MC dropout uncertainty')\n",
    "    # parser.add_argument('--progressive', action='store_true', help='Use progressive domain adaptation')\n",
    "    # parser.add_argument('--feature_align', action='store_true', help='Use feature alignment')\n",
    "    # args = parser.parse_args()\n",
    "    args = {}\n",
    "    args[\"source\"] = 'chase'\n",
    "    args[\"target\"] = 'rite'\n",
    "    args[\"mc_dropout\"] = True\n",
    "    args[\"multiscale\"] = False\n",
    "    args[\"progressive\"] = False\n",
    "    args[\"feature_align\"] = False\n",
    "    return args\n",
    "\n",
    "def enhanced_build_pseduo_augmentation(img):\n",
    "    \"\"\"Enhanced augmentation with more diversity\"\"\"\n",
    "    aug1 = st_transforms.ColorJitter(0.02, 0.02, 0.02, 0.01)\n",
    "    aug2 = st_transforms.RandomGrayscale(p=1.0)\n",
    "    aug3 = st_transforms.RandomSolarize(threshold=128.0, p=1.0)\n",
    "    aug4 = st_transforms.RandomAutocontrast(p=1.0)\n",
    "    aug5 = st_transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5))\n",
    "\n",
    "    aug_img1 = aug1(img).unsqueeze(0)\n",
    "    aug_img2 = aug2(img).unsqueeze(0)\n",
    "    aug_img3 = aug2(img).unsqueeze(0)\n",
    "    aug_img4 = aug4(img).unsqueeze(0)\n",
    "    aug_img5 = aug5(img).unsqueeze(0)\n",
    "    \n",
    "    aug_data = torch.cat([img.unsqueeze(0), aug_img1, aug_img2, aug_img3, aug_img4, aug_img5], dim=0)\n",
    "    return aug_data\n",
    "\n",
    "def enhanced_uncert_voting(aug_output, confidence_threshold=0.7):\n",
    "    \"\"\"Enhanced uncertainty voting with better confidence estimation\"\"\"\n",
    "    aug_all_prob = []\n",
    "    aug_all_ent = []\n",
    "    \n",
    "    for i in range(1, len(aug_output)):\n",
    "        prob = torch.sigmoid(aug_output[i])\n",
    "        aug_all_prob.append(prob)\n",
    "        entropy = -(prob * torch.log(prob + 1e-8) + (1-prob) * torch.log(1-prob + 1e-8))\n",
    "        aug_all_ent.append(entropy)\n",
    "    \n",
    "    no_aug_prob = torch.sigmoid(aug_output[0])\n",
    "    no_aug_entropy = -(no_aug_prob * torch.log(no_aug_prob + 1e-8) + \n",
    "                      (1-no_aug_prob) * torch.log(1-no_aug_prob + 1e-8))\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    if aug_all_prob:\n",
    "        aug_prob_mean = sum(aug_all_prob) / len(aug_all_prob)\n",
    "        aug_entropy_mean = sum(aug_all_ent) / len(aug_all_ent)\n",
    "        \n",
    "        # Combined confidence based on entropy\n",
    "        total_entropy = (no_aug_entropy + aug_entropy_mean) / 2\n",
    "        confidence_map = 1.0 - (total_entropy / np.log(2))  # Normalize\n",
    "        \n",
    "        # High confidence mask\n",
    "        high_conf_mask = confidence_map > confidence_threshold\n",
    "        \n",
    "        # Dynamic thresholding\n",
    "        ensemble_prob = (no_aug_prob + aug_prob_mean) / 2\n",
    "        adaptive_thresh = ensemble_prob.mean() + 0.1 * ensemble_prob.std()\n",
    "        adaptive_thresh = torch.clamp(adaptive_thresh, 0.3, 0.7)\n",
    "        \n",
    "        pseudo_label = (ensemble_prob > adaptive_thresh).float()\n",
    "        pseudo_label = pseudo_label * high_conf_mask.float()\n",
    "        \n",
    "        return pseudo_label.unsqueeze(0), confidence_map.unsqueeze(0)\n",
    "    else:\n",
    "        # Fallback to original\n",
    "        pseudo_label = dynamic_threshold_label(no_aug_prob)\n",
    "        confidence = 1.0 - (no_aug_entropy / np.log(2))\n",
    "        return pseudo_label.unsqueeze(0), confidence.unsqueeze(0)\n",
    "\n",
    "def dynamic_threshold_label(prob, alpha=0.3, min_thresh=0.3, max_thresh=0.8):\n",
    "    \"\"\"Enhanced dynamic threshold with better adaptation\"\"\"\n",
    "    dims = tuple(range(1, len(prob.shape)))\n",
    "    mean_prob = prob.mean(dim=dims, keepdim=True)\n",
    "    std_prob = prob.std(dim=dims, keepdim=True)\n",
    "    \n",
    "    thresh = mean_prob + alpha * std_prob\n",
    "    thresh = torch.clamp(thresh, min=min_thresh, max=max_thresh)\n",
    "    \n",
    "    pseudo_label = (prob >= thresh).float()\n",
    "    return pseudo_label\n",
    "\n",
    "@torch.jit.script\n",
    "def enhanced_sigmoid_entropy_loss(x: torch.Tensor, weight: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"Enhanced entropy loss with weighting\"\"\"\n",
    "    entropy = -(x*torch.log(x + 1e-8) + (1-x)*torch.log(1-x + 1e-8))\n",
    "    return weight * entropy.mean()\n",
    "\n",
    "def enhanced_consistency_loss(msrc_feat, tgt_feat, weights=None):\n",
    "    \"\"\"Enhanced consistency loss with optional weighting\"\"\"\n",
    "    if weights is None:\n",
    "        weights = [1.0] * min(len(msrc_feat), len(tgt_feat))\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for i, weight in enumerate(weights):\n",
    "        if i < len(msrc_feat) and i < len(tgt_feat):\n",
    "            total_loss += weight * loss_fn(tgt_feat[i], msrc_feat[i])\n",
    "    \n",
    "    return total_loss / len(weights)\n",
    "\n",
    "def enhanced_sfuda_target(config, train_loader, pseudo_model, msrc_model, criterion, optimizer, \n",
    "                         multiscale=None, mc_dropout=None, progressive=None, epoch=0):\n",
    "    \"\"\"Enhanced SFUDA target adaptation with academic improvements\"\"\"\n",
    "    avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter()}\n",
    "    pseudo_model.eval()\n",
    "    msrc_model.train()\n",
    "    pbar = tqdm(total=len(train_loader))\n",
    "\n",
    "    # Progressive threshold\n",
    "    confidence_thresh = 0.7\n",
    "    if progressive:\n",
    "        confidence_thresh = progressive.get_confidence_threshold(epoch)\n",
    "\n",
    "    for input, target, path in train_loader:\n",
    "        # Enhanced augmentation\n",
    "        aug_input = enhanced_build_pseduo_augmentation(input.squeeze(0))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if multiscale:\n",
    "                # Multi-scale pseudo-labeling\n",
    "                ps_output, conf_map = multiscale.generate_multiscale_pseudo_labels(\n",
    "                    pseudo_model, input.to(device)\n",
    "                )\n",
    "                ps_output = ps_output.unsqueeze(0)\n",
    "                conf_map = conf_map.unsqueeze(0)\n",
    "            elif mc_dropout:\n",
    "                # MC-Dropout uncertainty\n",
    "                mean_pred, uncertainty = mc_dropout.estimate_uncertainty(\n",
    "                    pseudo_model, input.to(device)\n",
    "                )\n",
    "                ps_output = mean_pred\n",
    "                conf_map = 1.0 / (1.0 + uncertainty)  # Inverse uncertainty as confidence\n",
    "            else:\n",
    "                # Enhanced voting\n",
    "                aug_output = pseudo_model(aug_input.to(device))\n",
    "                ps_output, conf_map = enhanced_uncert_voting(aug_output.detach(), confidence_thresh)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = msrc_model(aug_input.to(device))\n",
    "        \n",
    "        # Enhanced losses\n",
    "        if multiscale or mc_dropout:\n",
    "            # Confidence-weighted loss\n",
    "            seg_loss = confidence_weighted_loss(\n",
    "                torch.sigmoid(output), \n",
    "                ps_output.repeat(aug_input.size(0), 1, 1, 1).to(device),\n",
    "                conf_map.repeat(aug_input.size(0), 1, 1, 1).to(device)\n",
    "            )\n",
    "        else:\n",
    "            seg_loss = criterion(output.to(device), ps_output.repeat(aug_input.size(0), 1, 1, 1).to(device))\n",
    "        \n",
    "        # Enhanced entropy loss with adaptive weighting\n",
    "        ent_weight = 0.1 if epoch < config.get('stage1', 50) // 2 else 0.05\n",
    "        ent_loss = enhanced_sigmoid_entropy_loss(torch.sigmoid(output), ent_weight)\n",
    "        \n",
    "        loss = seg_loss + ent_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iou, dice = iou_score(output, target.to(device))\n",
    "        avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "        avg_meters['iou'].update(iou, input.size(0))\n",
    "\n",
    "        postfix = OrderedDict([\n",
    "            ('loss', avg_meters['loss'].avg),\n",
    "            ('iou', avg_meters['iou'].avg),\n",
    "            ('conf_thresh', confidence_thresh if progressive else 0.7),\n",
    "        ])\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "def enhanced_sfuda_task(train_loader, msrc_model, tgt_model, criterion, optimizer, \n",
    "                       feature_align=False, mc_dropout=None, epoch=0):\n",
    "    \"\"\"Enhanced SFUDA task adaptation with feature alignment\"\"\"\n",
    "    avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter()}\n",
    "    msrc_model.eval()\n",
    "    tgt_model.train()\n",
    "    pbar = tqdm(total=len(train_loader))\n",
    "\n",
    "    for input, target, _ in train_loader:\n",
    "        w_input = input.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        image_strong_aug = build_strong_augmentation(input.squeeze(0))\n",
    "        s_input = image_strong_aug.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if hasattr(msrc_model, 'forward') and 'mode' in msrc_model.forward.__code__.co_varnames:\n",
    "                w_output, msrc_feat = msrc_model(w_input, mode='const')\n",
    "            else:\n",
    "                w_output = msrc_model(w_input)\n",
    "                msrc_feat = []\n",
    "            \n",
    "            if mc_dropout:\n",
    "                ps_output, uncertainty = mc_dropout.estimate_uncertainty(msrc_model, w_input)\n",
    "                confidence = 1.0 / (1.0 + uncertainty)\n",
    "            else:\n",
    "                ps_output = torch.sigmoid(w_output).detach()\n",
    "                # Simple confidence based on distance from 0.5\n",
    "                confidence = 1.0 - 2.0 * torch.abs(ps_output - 0.5)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if hasattr(tgt_model, 'forward') and 'mode' in tgt_model.forward.__code__.co_varnames:\n",
    "            output, tgt_feat = tgt_model(s_input, mode='const')\n",
    "        else:\n",
    "            output = tgt_model(s_input)\n",
    "            tgt_feat = []\n",
    "\n",
    "        # Enhanced segmentation loss\n",
    "        if mc_dropout:\n",
    "            seg_loss = confidence_weighted_loss(torch.sigmoid(output), ps_output, confidence)\n",
    "        else:\n",
    "            # Confidence masking\n",
    "            conf_mask = confidence > 0.6\n",
    "            masked_pred = torch.sigmoid(output) * conf_mask\n",
    "            masked_target = ps_output * conf_mask\n",
    "            seg_loss = F.mse_loss(masked_pred, masked_target)\n",
    "\n",
    "        total_loss = seg_loss\n",
    "\n",
    "        # Feature alignment loss\n",
    "        if feature_align and msrc_feat and tgt_feat:\n",
    "            align_loss = feature_alignment_loss(msrc_feat, tgt_feat)\n",
    "            total_loss += 0.1 * align_loss\n",
    "\n",
    "        # Enhanced consistency loss\n",
    "        if msrc_feat and tgt_feat:\n",
    "            const_loss = enhanced_consistency_loss(msrc_feat, tgt_feat)\n",
    "            total_loss += 0.1 * const_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iou, dice = iou_score(output, target)\n",
    "        avg_meters['loss'].update(total_loss.item(), input.size(0))\n",
    "        avg_meters['iou'].update(iou, input.size(0))\n",
    "\n",
    "        postfix = OrderedDict([\n",
    "            ('loss', avg_meters['loss'].avg),\n",
    "            ('iou', avg_meters['iou'].avg),\n",
    "        ])\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Enhanced teacher model update with adaptive rate\n",
    "        update_rate = 0.999 if avg_meters['iou'].avg > 0.7 else 0.99\n",
    "        new_msrc_dict = update_teacher_model(tgt_model, msrc_model, keep_rate=update_rate)\n",
    "        msrc_model.load_state_dict(new_msrc_dict)\n",
    "        \n",
    "    pbar.close()\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg)])\n",
    "\n",
    "# ============= KEEP EXISTING FUNCTIONS =============\n",
    "\n",
    "def build_strong_augmentation(img):\n",
    "    \"\"\"Enhanced strong augmentation\"\"\"\n",
    "    augmentation = []\n",
    "    augmentation.append(st_transforms.RandomApply([st_transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8))\n",
    "    augmentation.append(st_transforms.RandomGrayscale(p=0.2))\n",
    "    augmentation.append(st_transforms.RandomApply([st_transforms.GaussianBlur(3, sigma=(0.1, 2.0))], p=0.5))\n",
    "    strong_aug = st_transforms.Compose(augmentation)\n",
    "    s_input = strong_aug(img)\n",
    "    return s_input\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_teacher_model(model_student, model_teacher, keep_rate=0.996):\n",
    "    student_model_dict = model_student.state_dict()\n",
    "    new_teacher_dict = OrderedDict()\n",
    "    for key, value in model_teacher.state_dict().items():\n",
    "        if key in student_model_dict.keys():\n",
    "            new_teacher_dict[key] = (\n",
    "                student_model_dict[key] * (1 - keep_rate) + value * keep_rate\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"{} is not found in student model\".format(key))\n",
    "    return new_teacher_dict\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    avg_meters = {'loss': AverageMeter(), 'iou': AverageMeter(), 'dice': AverageMeter()}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=len(val_loader))\n",
    "        for input, target, meta in val_loader:\n",
    "            input = input.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            iou, dice = iou_score(output, target)\n",
    "            avg_meters['loss'].update(loss.item(), input.size(0))\n",
    "            avg_meters['iou'].update(iou, input.size(0))\n",
    "            avg_meters['dice'].update(dice, input.size(0))\n",
    "            postfix = OrderedDict([\n",
    "                ('loss', avg_meters['loss'].avg),\n",
    "                ('iou', avg_meters['iou'].avg),\n",
    "                ('dice', avg_meters['dice'].avg)\n",
    "            ])\n",
    "            pbar.set_postfix(postfix)\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "    return OrderedDict([('loss', avg_meters['loss'].avg),\n",
    "                        ('iou', avg_meters['iou'].avg),\n",
    "                        ('dice', avg_meters['dice'].avg)])\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "\n",
    "    config_file = \"config_\" + args[\"target\"]\n",
    "    with open('models/%s/%s.yml' % (args[\"source\"], config_file), 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    # Initialize academic improvements\n",
    "    multiscale = MultiScaleConsistency() if args[\"multiscale\"] else None\n",
    "    mc_dropout = MCDropoutUncertainty() if args[\"mc_dropout\"] else None\n",
    "    progressive = ProgressiveDomainAdaptation(\n",
    "        total_epochs=config.get('stage1', 50)\n",
    "    ) if args[\"progressive\"] else None\n",
    "\n",
    "    print(\"Academic improvements enabled:\")\n",
    "    # Data loading (unchanged)\n",
    "    train_img_ids = glob(os.path.join('inputs', args[\"target\"], 'train','images', '*' + config['img_ext']))\n",
    "    train_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in train_img_ids]\n",
    "\n",
    "    val_img_ids = glob(os.path.join('inputs', args[\"target\"], 'test','images', '*' + config['img_ext']))\n",
    "    val_img_ids = [os.path.splitext(os.path.basename(p))[0] for p in val_img_ids]\n",
    "\n",
    "    train_transform = Compose([\n",
    "        RandomRotate90(),\n",
    "        transforms.Flip(),\n",
    "        Resize(config['input_h'], config['input_w']),\n",
    "        transforms.Normalize(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = Dataset(\n",
    "        img_ids=train_img_ids,\n",
    "        img_dir=os.path.join('inputs', args[\"target\"], 'train','images'),\n",
    "        mask_dir=os.path.join('inputs', args[\"target\"], 'train','masks'),\n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=train_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=1, shuffle=True,\n",
    "        num_workers=config['num_workers'], drop_last=True)\n",
    "\n",
    "    val_transform = Compose([\n",
    "        Resize(config['input_h'], config['input_w']),\n",
    "        transforms.Normalize(),\n",
    "    ])\n",
    "\n",
    "    val_dataset = Dataset(\n",
    "        img_ids=val_img_ids,\n",
    "        img_dir=os.path.join('inputs', args[\"target\"],'test', 'images'),\n",
    "        mask_dir=os.path.join('inputs', args[\"target\"],'test', 'masks'),\n",
    "        img_ext=config['img_ext'],\n",
    "        mask_ext=config['mask_ext'],\n",
    "        num_classes=config['num_classes'],\n",
    "        transform=val_transform)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=1, shuffle=False,\n",
    "        num_workers=config['num_workers'], drop_last=False)\n",
    "\n",
    "    # Model setup (unchanged)\n",
    "    print(\"Creating model %s...!!!\" % config['arch'])\n",
    "    print(\"Loading source trained model...!!!\")\n",
    "    \n",
    "    msrc_model = archs.__dict__[config['arch']](config['num_classes'],\n",
    "                                               config['input_channels'],\n",
    "                                               config['deep_supervision'])\n",
    "    msrc_model.load_state_dict(torch.load('models/%s/model.pth'%config['name'], map_location=\"mps\"))\n",
    "    msrc_model.to(device)\n",
    "    msrc_model.train()\n",
    "    print(\"Successfully loaded source trained model...!!!\")\n",
    "\n",
    "    tgt_model = archs.__dict__[config['arch']](config['num_classes'],\n",
    "                                              config['input_channels'],\n",
    "                                              config['deep_supervision'])\n",
    "    tgt_model.to(device)\n",
    "    tgt_model.train()\n",
    "\n",
    "    src_params = filter(lambda p: p.requires_grad, msrc_model.parameters())\n",
    "    src_optimizer = optim.Adam(src_params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    tgt_params = filter(lambda p: p.requires_grad, tgt_model.parameters())\n",
    "    tgt_optimizer = optim.Adam(tgt_params, lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    for c in range(config['num_classes']):\n",
    "        os.makedirs(os.path.join('outputs', config['name'], str(c)), exist_ok=True)\n",
    "    \n",
    "    pseudo_model = archs.__dict__[config['arch']](config['num_classes'],\n",
    "                                                 config['input_channels'],\n",
    "                                                 config['deep_supervision'])\n",
    "    pretrained_dict = msrc_model.state_dict()\n",
    "    pseudo_model.load_state_dict(pretrained_dict)\n",
    "    pseudo_model.to(device)\n",
    "    pseudo_model.eval()\n",
    "\n",
    "    criterion = losses.__dict__[config['loss']]().to(device)\n",
    "    \n",
    "    print(\"\\nTarget specific adaptation with enhancements...!!!\")\n",
    "    best_iou = 0.0\n",
    "    for epoch in range(config['stage1']):\n",
    "        train_log = enhanced_sfuda_target(\n",
    "            config, train_loader, pseudo_model, msrc_model, criterion, src_optimizer,\n",
    "            multiscale=multiscale, mc_dropout=mc_dropout, progressive=progressive, epoch=epoch\n",
    "        )\n",
    "        \n",
    "        print('Epoch %d - train_loss %.4f - train_iou %.4f' % \n",
    "              (epoch, train_log['loss'], train_log['iou']))\n",
    "        \n",
    "        # Validation every 10 epochs\n",
    "        if epoch % 10 == 0 or epoch == config['stage1'] - 1:\n",
    "            val_log = validate(val_loader, msrc_model, criterion)\n",
    "            print('Validation - dice: %.4f' % val_log['dice'])\n",
    "            \n",
    "            if val_log['dice'] > best_iou:\n",
    "                best_iou = val_log['dice']\n",
    "                torch.save(msrc_model.state_dict(), \n",
    "                          f'models/{config[\"name\"]}/best_stage1_model.pth')\n",
    "\n",
    "    msrc_model.eval()\n",
    "    pretrained_dict = msrc_model.state_dict()\n",
    "    tgt_model.load_state_dict(pretrained_dict)\n",
    "    tgt_model.to(device)\n",
    "    tgt_model.train()\n",
    "\n",
    "    print(\"\\nTask specific adaptation with enhancements...!!!\")\n",
    "    best_dice = 0.0\n",
    "    for epoch in range(config['stage2']):\n",
    "        train_log = enhanced_sfuda_task(\n",
    "            train_loader, msrc_model, tgt_model, criterion, tgt_optimizer,\n",
    "            feature_align=args.feature_align, mc_dropout=mc_dropout, epoch=epoch\n",
    "        )\n",
    "        \n",
    "        print('Epoch %d - train_loss %.4f - train_iou %.4f' % \n",
    "              (epoch, train_log['loss'], train_log['iou']))\n",
    "        \n",
    "        # Validation every 10 epochs\n",
    "        if epoch % 10 == 0 or epoch == config['stage2'] - 1:\n",
    "            val_log = validate(val_loader, tgt_model, criterion)\n",
    "            print('Validation - dice: %.4f' % val_log['dice'])\n",
    "            \n",
    "            if val_log['dice'] > best_dice:\n",
    "                best_dice = val_log['dice']\n",
    "                torch.save(tgt_model.state_dict(), \n",
    "                          f'models/{config[\"name\"]}/best_final_model.pth')\n",
    "    \n",
    "    print(\"\\nPerforming final adapted target model evaluation...!!!\")\n",
    "    val_log = validate(val_loader, tgt_model, criterion)\n",
    "    print('Final adapted target model dice: %.4f' % val_log['dice'])\n",
    "    print('Best dice achieved: %.4f' % best_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "063a16c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic improvements enabled:\n",
      "Creating model UNet...!!!\n",
      "Loading source trained model...!!!\n",
      "Successfully loaded source trained model...!!!\n",
      "\n",
      "Target specific adaptation with enhancements...!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:09<00:04,  1.71it/s, loss=0.000139, iou=0.0194, conf_thresh=0.7]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 576\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    574\u001b[0m best_iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage1\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m--> 576\u001b[0m     train_log \u001b[38;5;241m=\u001b[39m \u001b[43menhanced_sfuda_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpseudo_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsrc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_optimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmultiscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmc_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmc_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogressive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogressive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m - train_loss \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m - train_iou \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \n\u001b[1;32m    582\u001b[0m           (epoch, train_log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], train_log[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;66;03m# Validation every 10 epochs\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 326\u001b[0m, in \u001b[0;36menhanced_sfuda_target\u001b[0;34m(config, train_loader, pseudo_model, msrc_model, criterion, optimizer, multiscale, mc_dropout, progressive, epoch)\u001b[0m\n\u001b[1;32m    323\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    324\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 326\u001b[0m iou, dice \u001b[38;5;241m=\u001b[39m iou_score(output, \u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    327\u001b[0m avg_meters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m    328\u001b[0m avg_meters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miou\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(iou, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a24eee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
